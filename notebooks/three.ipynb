{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4bab76c-2c29-4ad9-91bf-b63b042dfe89",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 40px; color: red; text-align: center;\">RECURRENT NEURAL NETWORKS(RNNs)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108c8627-31c3-4123-89a5-31825055efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52dae018-bee2-4520-9b14-069ef4078c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Dataset Class\n",
    "class CremaDataset(Dataset):\n",
    "    def __init__(self, data_path, emotion=[\"ANG\", \"SAD\", \"DIS\", \"NEU\", \"HAP\", \"FEA\"], max_len=200):\n",
    "        self.data = [] # Store processed MFCCs from audio\n",
    "        self.labels = [] # Store emotion classes(as intergers)\n",
    "        self.emotion_map = {\"ANG\": 0, \"SAD\": 1, \"DIS\": 2, \"NEU\": 3, \"HAP\": 4, \"FEA\": 5} # Map emotions into numbers\n",
    "        self.max_len = max_len # store max length for trim/pad\n",
    "\n",
    "        for file in os.listdir(data_path):\n",
    "            # Check for the right file type\n",
    "            if not file.endswith('.wav'):\n",
    "                continue \n",
    "            # Get emotion\n",
    "            emotion = file.split('_')[2]\n",
    "            # Ensure it's all in the map\n",
    "            if emotion not in self.emotion_map:\n",
    "                continue\n",
    "            # Extract MFCCs\n",
    "            path = os.path.join(data_path,file)\n",
    "            y, sr = librosa.load(path, sr=16000)\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T # (time, 40)\n",
    "            # Trim or pad\n",
    "            if len(mfcc) < max_len:\n",
    "                pad_width = max_len - len(mfcc)\n",
    "                mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)))\n",
    "            else:\n",
    "                mfcc = mfcc[:max_len]\n",
    "            # Normalize\n",
    "            mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
    "            # Append data and labels\n",
    "            self.data.append(mfcc)\n",
    "            self.labels.append(self.emotion_map[emotion])\n",
    "    # Check the length of data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    # Conversion for DataLoader, then model training\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32) # mfcc matrix into tensor float\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long) # labels into tensor long\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab816188-3de9-4e69-a7d5-73721c1a190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model architecture\n",
    "class EmotionRNN(nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_size=128, num_layers=2, num_classes=6):\n",
    "        # Call the constructor for the base class nn.Module\n",
    "        super(EmotionRNN, self).__init__()\n",
    "        # Instantiate the architecture\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    # The forward pass\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        # output shape: (batch_size, seq_len, hidden_size)\n",
    "        # We take the last time step's output\n",
    "        last_output = output[:, -1, :] # Shape: (batch_size, hidden_size)\n",
    "        logits = self.fc(hidden[-1])  # hidden shape: (num_layers, batch, hidden_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "163d4b5c-f009-4d8f-b347-82bf6b043d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN x RNN architecture\n",
    "class EmotionCRNN(nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_size=128, num_layers=2, num_classes=6, dropout_rate=0.3):\n",
    "        # Call the constructor for the base class nn.Module\n",
    "        super(EmotionCRNN, self).__init__()\n",
    "        # CNN layer: Extract local features from MFCCs\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1), # Output: (batch, 16, time, 40)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)), # Output: (batch, 16, time//2, 20)\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1),  # (B, 32, T/2, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),                  # (B, 32, T/4, 10)\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        self.rnn_input_size = 32 * 10  # After 2 MaxPool2d layers\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0.0\n",
    "        )\n",
    "        # Linear layer doubles hidden_size if bidirectional\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # Add channel dim for processing by CNN\n",
    "        # CNN Feature extraction\n",
    "        x = self.cnn(x) # (batch, channels, new_time, new_mfcc)\n",
    "        # Change shape for RNN\n",
    "        x = x.permute(0, 2, 1, 3)  # (batch, new_time, channels, new_mfcc)\n",
    "        x = x.contiguous().view(x.size(0), x.size(1), -1)  # (batch, new_time, channels * new_mfcc)\n",
    "        # Pass through RNN\n",
    "        output, (hidden, _) = self.rnn(x)\n",
    "\n",
    "        # hidden shape: (num_layers * 2, B, hidden_size) -> concat last layer's forward & backward\n",
    "        last_layer_forward = hidden[-2]  # (B, hidden_size)\n",
    "        last_layer_backward = hidden[-1]  # (B, hidden_size)\n",
    "        combined_hidden = torch.cat((last_layer_forward, last_layer_backward), dim=1)  # (B, hidden_size*2)\n",
    "\n",
    "        logits = self.fc(combined_hidden)  # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f9876d5-b589-4489-abae-f43eec3db965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.605916\n",
      "Epoch 2 Loss: 1.468640\n",
      "Epoch 3 Loss: 1.401549\n",
      "Epoch 4 Loss: 1.370378\n",
      "Epoch 5 Loss: 1.321991\n",
      "Epoch 6 Loss: 1.271911\n",
      "Epoch 7 Loss: 1.240535\n",
      "Epoch 8 Loss: 1.222128\n",
      "Epoch 9 Loss: 1.196238\n",
      "Epoch 10 Loss: 1.152558\n",
      "Epoch 11 Loss: 1.127106\n",
      "Epoch 12 Loss: 1.099697\n",
      "Epoch 13 Loss: 1.098837\n",
      "Epoch 14 Loss: 1.073986\n",
      "Epoch 15 Loss: 1.038205\n",
      "Epoch 16 Loss: 1.010409\n",
      "Epoch 17 Loss: 0.990869\n",
      "Epoch 18 Loss: 0.953155\n",
      "Epoch 19 Loss: 0.939073\n",
      "Epoch 20 Loss: 0.916111\n",
      "Epoch 21 Loss: 0.897056\n",
      "Epoch 22 Loss: 0.878602\n",
      "Epoch 23 Loss: 0.891754\n",
      "Epoch 24 Loss: 0.855107\n",
      "Epoch 25 Loss: 0.838787\n",
      "Epoch 26 Loss: 0.811167\n",
      "Epoch 27 Loss: 0.800190\n",
      "Epoch 28 Loss: 0.768018\n",
      "Epoch 29 Loss: 0.745192\n",
      "Epoch 30 Loss: 0.736223\n"
     ]
    }
   ],
   "source": [
    "# Process Dataset\n",
    "dataset = CremaDataset('../data/')\n",
    "# Split into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_data, val_data = random_split(dataset, [train_size, (len(dataset) - train_size)])\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "# The model, loss function & Optimizer\n",
    "model = EmotionCRNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Training loop begins\n",
    "for epoch in range(30):\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "    # Initialize loss for each epoch\n",
    "    total_loss = 0\n",
    "    # Mini-batch training\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Logits pre-softmax\n",
    "        y_pred = model(x_batch)\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        # Clear out gradients from last calculation to prevent accummulation\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update model parameters(weights and biases)\n",
    "        optimizer.step()\n",
    "        # Apply current epoch loss to total\n",
    "        total_loss += loss.item()\n",
    "    # Log progress\n",
    "    print(f'Epoch {epoch+1} Loss: {total_loss / len(train_loader):4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f52714c-a077-4df8-b6ae-655cb739b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.85%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "# Switch model to evaluate mode\n",
    "model.eval()\n",
    "# Initialize counters to keep track of model accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "# Disable gradient tracking\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        # Forward pass through model\n",
    "        outputs = model(x_batch)\n",
    "        # Pick the predicted emotion label\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # Correct predictions in this batch and add to the running total\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        # Add to number of samples ni this batch to total\n",
    "        total += y_batch.size(0)\n",
    "# Display final accuracy\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
