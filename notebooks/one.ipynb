{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056e5ff-4c91-40ea-9682-403e6320bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm as tqdm\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c8705-b092-4d72-95d3-a6db95eaee10",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 30px; color: red;\">Audio Data Loading & Basic Inspection</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3e9cf-e765-4791-951a-3bf6be4fb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample\n",
    "data, sr = librosa.load(\"../data/1001_DFA_DIS_XX.wav\", sr = None, mono=False)\n",
    "\n",
    "# Plot waveform\n",
    "plt.figure(figsize=(15, 6))\n",
    "librosa.display.waveshow(y=data, x_axis='time', sr=sr)\n",
    "plt.title(\"Waveform Example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa6551-b14c-495d-bcbe-b086ea25374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Audio\n",
    "ipd.Audio(\"../data/1001_DFA_DIS_XX.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50dbc9-7902-4fcf-bb06-4486ccc2ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of channels\n",
    "if data.ndim == 1:\n",
    "    print(\"File is a Mono Channel\")\n",
    "elif data.ndim == 2:\n",
    "    print(\"File is Stereo Channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7b524-2d16-4dbe-9845-9fdcd728ae9c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 30px; color: red;\">Exploratory Data Analysis</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215e3e9-c15d-4cea-adbc-b56a198d94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Class Imbalance\n",
    "emotion_map = {\n",
    "        'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful',\n",
    "        'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'\n",
    "}\n",
    "emotions = []\n",
    "for dirpath, dirnames, filenames in os.walk(\"../data\"):\n",
    "    for file in filenames:\n",
    "        emotion = emotion_map[file.split('_')[-2]]\n",
    "        emotions.append(emotion)\n",
    "df = pd.DataFrame(emotions)\n",
    "df.columns = [\"emotion\"]\n",
    "df[\"emotion\"].value_counts().plot(kind='bar')\n",
    "plt.title(\"Class Balance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b615394-6578-4c98-a64e-5af1f186c8e9",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- All classes balanced except neutral. Consider resampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0547b3-82f7-4682-a7da-0b5100734926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Audio for each emotion\n",
    "ipd.Audio(\"../data/1001_DFA_ANG_XX.wav\") # Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293f60b-9a80-430b-9ebe-aaa245f57ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"../data/1001_DFA_FEA_XX.wav\") # Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01795a5-15e4-4d81-83d6-c2e7e2f66491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"../data/1001_IEO_DIS_HI.wav\") # Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1f158-9d74-4df2-af4b-d622b1382106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"../data/1001_DFA_HAP_XX.wav\") # Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae23acb-20d7-4570-b921-785ae33b1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"../data/1001_IEO_SAD_MD.wav\") # Sad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881a67c-b364-4dd6-81e3-e20fd749fa75",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- Audio length is **NOT CONSISTENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b955593-5aec-49c8-aef1-685ac122185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_length(y, target_len = sr*3):\n",
    "    # Target is 3 seconds\n",
    "    if len(y) > target_len:\n",
    "        y_trunc = y[:target_len]\n",
    "        return y_trunc\n",
    "    else:\n",
    "        y_pad = np.pad(y, (0, target_len - len(y)))\n",
    "        return y_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057a23d-dc48-4577-8c66-db5a8a873568",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 25px; color: green;\">Waveform and Spectrogram Samples</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1dbb6-f04c-4ecf-86b4-4aba250afbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Samples\n",
    "fearful = \"../data/1001_DFA_FEA_XX.wav\"\n",
    "happy = \"../data/1001_IEO_HAP_HI.wav\"\n",
    "sad = \"../data/1001_DFA_SAD_XX.wav\"\n",
    "angry = \"../data/1001_DFA_ANG_XX.wav\"\n",
    "neutral = \"../data/1001_DFA_NEU_XX.wav\"\n",
    "disgust = \"../data/1001_DFA_DIS_XX.wav\"\n",
    "# Put in list for easy iteration\n",
    "samples_list = [fearful, happy, sad, angry, neutral, disgust]\n",
    "samples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff757a2-d630-4b75-b1c5-2c4134424144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(path):\n",
    "    # Get the emotion\n",
    "    file = os.path.basename(path)\n",
    "    emotion = emotion_map[os.path.basename(path).split(\"_\")[-2]]\n",
    "    \n",
    "    # Plot waveform\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(f\"{emotion.title()} Waveform\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    X = librosa.stft(y)\n",
    "    Xdb = librosa.amplitude_to_db(np.abs(X))\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.title(f\"{emotion.title()} Spectrogram\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot mel-spectrogram\n",
    "    # Initiate Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "    # Scale to db\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    # Mel Spectrogram Plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    librosa.display.specshow(data=mel_db,\n",
    "                             sr=22050,\n",
    "                             hop_length=512,\n",
    "                             x_axis=\"time\",\n",
    "                             y_axis=\"mel\",\n",
    "                             cmap=\"viridis\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"{emotion.title()} Mel Spectrogram\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc002c4b-7a7e-464d-8746-095a2005a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples_list:\n",
    "    visualize_audio(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fd83f-7f16-41b5-81ce-994321367146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fix length function\n",
    "y, sr = librosa.load(neutral, sr=None)\n",
    "print(f'Initial length: {len(y)}')\n",
    "fixed_len = fix_length(y)\n",
    "print(f'Fixed length: {len(fixed_len)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c20bb-dac9-4aff-a851-24c4f92b65b8",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 25px; color: green;\">Resampling</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e1ed0-819f-4158-b1a8-5039926f921f",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- There's a slight imbalance. I'll have to oversample neutral class to bring everything level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ca20b-e77c-401c-8211-952de4d4548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference list for your data. Item e.g, (path, emotion)\n",
    "data = []\n",
    "for dirpath, dirnames, filenames in os.walk(\"../data\"):\n",
    "    for file in filenames:\n",
    "        path = os.path.join(dirpath, file)\n",
    "        speaker = file.split('_')[0] # Need to identify speaker since dataset is speaker-independent\n",
    "        emotion = emotion_map[file.split('_')[-2]]\n",
    "        data.append((path, speaker, emotion))\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcfd08-022f-4697-8277-6af522f90fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data into a dataframe\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.columns = [\"path\",\"speaker\", \"emotion\"]\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c3608-b4e9-4ab9-80f2-7870d9b1ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into majority and minority classes for resampling\n",
    "majority_df = data_df[data_df[\"emotion\"]!=\"neutral\"]\n",
    "minority_df = data_df[data_df[\"emotion\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c667160-f7af-4899-9768-4488943ac48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform oversampling\n",
    "neutral_upsampled = resample(\n",
    "    minority_df,\n",
    "    replace = True,\n",
    "    n_samples = 1271,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd38003-f6a9-4690-ac86-bd8d45e845c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine back to one df\n",
    "data_df = pd.concat([majority_df, neutral_upsampled])\n",
    "data_df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d695ba1-c0b6-40d4-9609-87460405eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c9d73-cc4f-4231-b805-d957e3549270",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- This dataset **IS NOT** inherently speaker dependent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af367596-c36c-4a35-952e-2c37107fdf0e",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 25px; color: green;\">Splitting</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e4c51-7693-4e58-adad-68679a4b5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
